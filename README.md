# Torchy
Built from scratch neural network library using only Numpy and PyTorch as reference to 

## Implemented
1. Layers
    - Linear (Dense) | With Kaimin parameter initialization
    - ReLU
2. Optimizations
   - SGD;
   - MomentumSGD;
3. Loss Functions
   - Cross-entropy Loss + Softmax


## TO DO (Until 10th august)
1. BatchNorm1d, BatchNorm2d
3. Convolution Layer (+padding, stride)
4. Activation functions (tanh, leaky relu)
5. Adam, RMSProp optimizations
