# Torchy
Built from scratch neural network library using only Numpy and PyTorch as reference to 

## Implemented
1. Layers
    - Linear (Dense) | With Kaimin parameter initialization
    - ReLU
2. Optimizations
   - SGD;
   - MomentumSGD;
   - Adam;
   - RMRSProp;
   - Adagrad
3. Loss Functions
   - Cross-entropy Loss + Softmax
4. Schedulers
   - StepLR;
   - ReduceLROnPlateau

## TO DO (MVP)
1. BatchNorm1d, BatchNorm2d
2. Dataloader and Dataset logic
3. Convolution Layer (+padding, stride)
4. Activation functions (tanh, leaky relu)
